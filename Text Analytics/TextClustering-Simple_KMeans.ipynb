{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text clustering with k-means\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create document list\n",
    "documents1 = [\"google pus\",\"google mail\", \"Google Translate app\", \"If you open 100 tab in google you get a smiley face.\",\"Best cat photo\",\"Climbing ninja cat.\",\"Impressed with google map feedback.\",\"Key promoter extension from Google Chrome.\"]\n",
    "documents = [\"k-means clustering is a method of vector quantization\",\"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression.\",\" cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.\",\"The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21)\t0.3528554929793508\n",
      "  (0, 9)\t0.43584673254990375\n",
      "  (0, 22)\t0.5528163151092931\n",
      "  (0, 41)\t0.43584673254990375\n",
      "  (0, 35)\t0.43584673254990375\n",
      "  (1, 41)\t0.18100388622274022\n",
      "  (1, 35)\t0.36200777244548044\n",
      "  (1, 43)\t0.2295804784785442\n",
      "  (1, 5)\t0.2295804784785442\n",
      "  (1, 39)\t0.18100388622274022\n",
      "  (1, 38)\t0.2295804784785442\n",
      "  (1, 33)\t0.2295804784785442\n",
      "  (1, 2)\t0.2295804784785442\n",
      "  (1, 24)\t0.2295804784785442\n",
      "  (1, 32)\t0.2295804784785442\n",
      "  (1, 14)\t0.2295804784785442\n",
      "  (1, 16)\t0.2295804784785442\n",
      "  (1, 15)\t0.2295804784785442\n",
      "  (1, 34)\t0.18100388622274022\n",
      "  (1, 42)\t0.2295804784785442\n",
      "  (1, 29)\t0.2295804784785442\n",
      "  (1, 40)\t0.2295804784785442\n",
      "  (1, 13)\t0.18100388622274022\n",
      "  (1, 11)\t0.2295804784785442\n",
      "  (2, 21)\t0.13641296406016137\n",
      "  :\t:\n",
      "  (2, 8)\t0.6411518054158896\n",
      "  (2, 3)\t0.2137172684719632\n",
      "  (2, 23)\t0.2137172684719632\n",
      "  (2, 0)\t0.2137172684719632\n",
      "  (2, 30)\t0.2137172684719632\n",
      "  (2, 28)\t0.2137172684719632\n",
      "  (2, 10)\t0.2137172684719632\n",
      "  (2, 27)\t0.2137172684719632\n",
      "  (2, 4)\t0.2137172684719632\n",
      "  (2, 25)\t0.1684971492467261\n",
      "  (2, 20)\t0.2137172684719632\n",
      "  (2, 37)\t0.2137172684719632\n",
      "  (3, 21)\t0.1870001543849191\n",
      "  (3, 39)\t0.23098239334980156\n",
      "  (3, 25)\t0.23098239334980156\n",
      "  (3, 1)\t0.29297187752151427\n",
      "  (3, 18)\t0.29297187752151427\n",
      "  (3, 36)\t0.29297187752151427\n",
      "  (3, 26)\t0.29297187752151427\n",
      "  (3, 7)\t0.29297187752151427\n",
      "  (3, 31)\t0.29297187752151427\n",
      "  (3, 19)\t0.29297187752151427\n",
      "  (3, 17)\t0.29297187752151427\n",
      "  (3, 6)\t0.29297187752151427\n",
      "  (3, 12)\t0.29297187752151427\n"
     ]
    }
   ],
   "source": [
    "#Transform the data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=3, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the clusters\n",
    "true_k = 3\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " quantization\n",
      " vector\n",
      " method\n",
      " clustering\n",
      " means\n",
      " originally\n",
      " allows\n",
      " classical\n",
      " compression\n",
      " density\n",
      "Cluster 1:\n",
      " popular\n",
      " classification\n",
      " learning\n",
      " neighbor\n",
      " confused\n",
      " loose\n",
      " classifier\n",
      " machine\n",
      " relationship\n",
      " algorithm\n",
      "Cluster 2:\n",
      " cluster\n",
      " aims\n",
      " observations\n",
      " analysis\n",
      " belongs\n",
      " serving\n",
      " clusters\n",
      " mean\n",
      " mining\n",
      " partition\n"
     ]
    }
   ],
   "source": [
    "#Profile the clusters\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prediction\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#Use model for prediction\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    " \n",
    "Y = vectorizer.transform([\"classification.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"Vector is there.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.transform([\"mean values\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
