{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Text Analytics\n",
    "Text analytics is the process of converting unstructured text to structured form and applying statistical/AI methods to discover hidden patterns, predcit the classes/labels as required. In addition summarization to understand the text faster is also a part of text analytics.\n",
    "The process of Text analytics consists of four stages. In the first stage the unstructured data is preprocessed and converted into standard format for analysis. In the second stage the different models are developed for various requirements as clustering, classification, Information extraction (IE), Topic Detection and summarization. In the third stage, the discovered knowedge is presented as per the requirments. In the fourth stage, the developed models will be used for scoring unseen data. The fourth step requires first and second step, similar to third step of presentation.\n",
    "\n",
    "This course is an introduction to text analytics. After the completion of this course, the participants will have/can:\n",
    "\n",
    "        •\tAbility to read text data and preprocess\n",
    "        •\tClassify the text automatically\n",
    "        •\tGroup the text/documents by similarity\n",
    "        •\tDetect the topics in texts\n",
    "        •\tAnalyze the social media for sentiment\n",
    "        •\tAnalyze the social media for networks/communities\n",
    "\n",
    "The topics covered inthis session will be as follows:\n",
    "1.Data preprocessing\n",
    "    - Text analytics\n",
    "    - Difference between NLP and Text analytics\n",
    "    - Accessing the texts and unstructured data\n",
    "    - Preprocessing steps\n",
    "            o\tTokenizing\n",
    "            o\tStemming\n",
    "            o\tLemmatization\n",
    "            o\tPoS tagging\n",
    "            o\tn-gram tagging\n",
    "            o\tStop words\n",
    "            o\tNormalization\n",
    "            o\tText cleaning – punctuation, symbols, gaps etc\n",
    "            o\tText enrichment – emoticons, exclamations\n",
    "            o\tLexicons for knowledge based analysis of text\n",
    "            o\tBoW format for text analysis\n",
    "            o\tTFIDF format for text analysis\n",
    "            o\tWord2Vector/Embedding format for text analysis\n",
    "            o\tText frequency analysis – unigrams, bigrams\n",
    "            o\tSemantic analysis\n",
    "            o\tStoring processed texts\n",
    "            o\tWeb scraping\n",
    "            o\tVisualizing text analysis results\n",
    "            o   Creating pipelines for analysis\n",
    "    •\tProject 1 – Lexicon based analysis for sentiments\n",
    "            o\tAFINN, SentiNet, WordNet, PubMed\n",
    "            o\tPolarity strength at sentence level, paragraph level\n",
    "            o\tVisualizing the results\n",
    "\n",
    "\n",
    "\n",
    "***References***\n",
    "\n",
    "The following books are highly referenced in this course, apart from several online materials as appropriate:\n",
    "\n",
    "        •\tFundamentals of Predictive Text Mining, By - Sholom M. Weiss, Nitin Indurkhya and Tong Zhang, Springer-Verlag London, 2015 \n",
    "        •\tNatural Language Processing with Python – Analyzing Text with the Natural Language Toolkit, By - Steven Bird, Ewan Klein, and Edward Loper,O'Reilly, 2017\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "04,July, 2018\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "sampleString = \"Hello World\"\n",
    "print(sampleString)\n",
    "timeToday = \"04,July, 2018\"\n",
    "print(timeToday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n",
      "['04', 'July', ' 2018']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "print(sampleString.split())\n",
    "print(timeToday.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Who is the PM of India? you know it!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'is', 'the', 'PM', 'of', 'India?', 'you', 'know', 'it!!']\n"
     ]
    }
   ],
   "source": [
    "print(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['who', 'is', 'the', 'of', 'you', 'know', 'it!!', '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who', 'is', 'the', 'of', 'you', 'know', 'it!!', '?']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PM', 'India?']\n"
     ]
    }
   ],
   "source": [
    "string = \"Who is the PM of India? you know it!!\"\n",
    "stop_words = ['who', 'is', 'the', 'of', 'you', 'know', 'it!!', '?']\n",
    "\n",
    "refined_str = []\n",
    "for word in string.split():\n",
    "    if word.lower() in stop_words:\n",
    "        pass\n",
    "    else:\n",
    "        refined_str.append(word)\n",
    "print(refined_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyz', 'is', 'to', 'abc', 'as', 'water', 'is', 'to', 'sea']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"xyz is to abc as water is to sea\"\n",
    "tokens = string.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['is', 'to', 'as', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xyz', 'abc', 'water', 'sea']\n"
     ]
    }
   ],
   "source": [
    "new_sent = []\n",
    "for word in tokens:\n",
    "    if (word.lower() in stopwords):\n",
    "        pass\n",
    "    else:\n",
    "        new_sent.append(word)\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India got it's independence in 1947. However, it is not yet clear, the progess of nation compared to China.\n"
     ]
    }
   ],
   "source": [
    "#Sentence tokenization\n",
    "sampleString2 = \"India got it's independence in 1947. However, it is not yet clear, the progess of nation compared to China.\"\n",
    "print(sampleString2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"India is the only democratic country. All laws are valid in India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India is the only democratic country', ' All laws are valid in India']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = para.split('.')\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"India got it's independence in 1947\", ' However, it is not yet clear, the progess of nation compared to China', '']\n"
     ]
    }
   ],
   "source": [
    "first_split = sampleString2.split(\".\")\n",
    "print(first_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"India got it's independence in 1947\"], [' However', ' it is not yet clear', ' the progess of nation compared to China'], ['']]\n"
     ]
    }
   ],
   "source": [
    "second_split = []\n",
    "for sent in first_split:\n",
    "    temp = sent.split(',')\n",
    "    second_split.append(temp)\n",
    "print(second_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'got', \"it's\", 'independence', 'in', '1947']\n",
      "['However']\n",
      "['it', 'is', 'not', 'yet', 'clear']\n",
      "['the', 'progess', 'of', 'nation', 'compared', 'to', 'China']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for xyz in second_split:\n",
    "    for sent in xyz:\n",
    "        print(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'an', 'Indian']\n",
      "['I', 'love', 'India']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "lakshmis_sentence = \"I am an Indian. I love India.\"\n",
    "for sent in lakshmis_sentence.split(\".\"):\n",
    "    print(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this's a sent tokenize test. this is sent two.is this sent three? sent 4 is cool! Now it’s your turn.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"this's a sent tokenize test\", ' this is sent two', 'is this sent three? sent 4 is cool! Now it’s your turn', '']\n"
     ]
    }
   ],
   "source": [
    "print(text.split(\".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the word frequency\n",
    "def wordfreq (str):\n",
    "    \"\"\"Function to generate the frequency distribution of the\n",
    "given text\"\"\"\n",
    "    print(str)\n",
    "    word_freq={}\n",
    "    for tok in str.split():\n",
    "        if tok.lower() in word_freq:\n",
    "            word_freq [tok.lower()]+=1\n",
    "        else:\n",
    "            word_freq [tok.lower()]=1\n",
    "    print(word_freq)\n",
    "wordfreq(\"This is a good sentence on good topic of this\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = ['A', 'B', 'A', 'C']\n",
    "dict = {}\n",
    "for element in lst:\n",
    "    if element in dict:\n",
    "        dict[element] = dict[element] +1\n",
    "    else:\n",
    "        dict[element] = 1\n",
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Is', 'A', 'Good', 'Sentence', 'On', 'good', 'topic', 'of', 'This']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'good', 'sentence', 'on', 'good', 'topic', 'of', 'this']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq={}\n",
    "str = \"This Is A Good Sentence On good topic of This\"\n",
    "tokens = str.split()\n",
    "print(tokens)\n",
    "tokens_low = []\n",
    "for token in tokens:\n",
    "     tokens_low.append(token.lower())\n",
    "tokens_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a good sentence on good topic of this\n",
      "['this', 'good', 'sentence', 'topic']\n"
     ]
    }
   ],
   "source": [
    "#Some of them are stop words..what do we do with them\n",
    "#Remove them\n",
    "stop_words = ('is', 'a', 'on', 'of')\n",
    "line = \"This is a good sentence on good topic of this\"\n",
    "word_list = []\n",
    "print(line)\n",
    "for tok in line.split():\n",
    "    if tok.lower() in stop_words:\n",
    "        pass\n",
    "    else:\n",
    "        if tok.lower() in word_list:\n",
    "            pass\n",
    "        else:\n",
    "            word_list.append(tok.lower())\n",
    "print(word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfreq (words):\n",
    "    \"\"\"Function to generated the frequency distribution of the\n",
    "given text\"\"\"\n",
    "    word_freq={}\n",
    "    for tok in words:\n",
    "        if tok.lower() in word_freq:\n",
    "            word_freq [tok.lower()]+=1\n",
    "        else:\n",
    "            word_freq [tok.lower()]=1\n",
    "    return word_freq\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PoS tagging\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag('Fruit flies like a banana'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one',)\n",
      "('two',)\n",
      "('three',)\n",
      "('four',)\n",
      "('one', 'two')\n",
      "('two', 'three')\n",
      "('three', 'four')\n",
      "('one', 'two', 'three')\n",
      "('two', 'three', 'four')\n"
     ]
    }
   ],
   "source": [
    "#ngram taggers\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "words = 'one two three four'.split(' ')\n",
    "#print(words)\n",
    "s = []\n",
    "for n in range(1, 4):\n",
    "    for ngram in ngrams(words, n):\n",
    "        print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game\n",
      "game\n",
      "game\n",
      "game\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookeri'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('cookery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooking'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'sentence', 'second', 'set', 'next']\n",
      "['First sentence', 'First second set', 'next set']\n",
      "[[1, 1, 0, 0, 0], [1, 0, 1, 1, 0], [0, 0, 0, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "#Bag of words\n",
    "docs = ['First sentence', 'First second set', 'next set']\n",
    "#Identify the unique words\n",
    "###For eack row, tokenize\n",
    "unique_words = []\n",
    "for row in docs:\n",
    "    tokens = row.split()\n",
    "    for token in tokens:\n",
    "        \n",
    "        if token in unique_words:\n",
    "            pass\n",
    "        else:\n",
    "            unique_words.append(token)\n",
    "print(unique_words)          \n",
    "\n",
    "bow = []\n",
    "for row in docs:\n",
    "    doc_to_bow = []\n",
    "    tokens = row.split()\n",
    "    for word in unique_words:\n",
    "        if word in tokens:\n",
    "            doc_to_bow.append(1)\n",
    "        else:\n",
    "            doc_to_bow.append(0) \n",
    "    bow.append(doc_to_bow)\n",
    "print(docs)\n",
    "print(bow)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
