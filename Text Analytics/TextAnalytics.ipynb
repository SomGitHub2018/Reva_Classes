{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning for natural language processing and text analytics involves using machine learning algorithms and “narrow” artificial intelligence (AI) to understand the meaning of text documents\n",
    "\n",
    "#### Tokenization\n",
    "is the act of breaking up a sequence of strings into pieces such as words, keywords, \n",
    "phrases, symbols and other elements called tokens. Tokens can be individual words, phrases or even whole sentences. \n",
    "In the process of tokenization, some characters like punctuation marks are discarded.\n",
    "\n",
    "#### Text analysis and Text mining\n",
    " Accessing the texts and unstructured data\n",
    "#### Text preprocessing \n",
    "- Preprocessing is an important task and critical step in Text mining, Natural Language Processing (NLP) and information retrieval (IR). In the area of Text Mining, data preprocessing used for extracting interesting and non-trivial and knowledge from unstructured text data.\n",
    "\n",
    "#### Tokenizing \n",
    "- Tokenization. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. ... A type is the class of all tokens containing the same character sequence.\n",
    "\n",
    "#### Stemming \n",
    "- Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. - ex: Hot Hotter Hotest\n",
    "\n",
    "#### Lemmatization \n",
    "- Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . ex: Good Better Best\n",
    "\n",
    "#### PoS tagging \n",
    "- (Part of Speech)POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition. This task is not straightforward, as a particular word may have a different part of speech based on the context in which the word is used.\n",
    "\n",
    "#### Stop words \n",
    "- In natural language processing, useless words (data), are referred to as stop words. Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "#### Embedding \n",
    "- Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "#### Normalization \n",
    "- Normalization is a process that converts a list of words to a more uniform sequence. Understand that the normalization process might also compromise an NLP task. Converting to lowercase letters can decrease the reliability of searches when the case is important.\n",
    "\n",
    "#### Text cleaning – punctuation, symbols, gaps etc\n",
    "\n",
    "#### Text enrichment – emoticons, exclamations\n",
    "\n",
    "#### Lexicons \n",
    "- A corpus is a lexical resource that usually comprises words and some semantic info on those words, such as a dictionary. A corpus contains unstructured natural language text, and is used to apply nlp tasks on in attempt to enable machines to better understand this text.\n",
    "\n",
    "#### Corpus \n",
    "- In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful.\n",
    "\n",
    "#### BoW format for text analysis \n",
    "- Bag of Words (BOW): We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary. Another representation can be count the number of times each word appears in a document. \n",
    "\n",
    "- The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
    "\n",
    "- Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "\n",
    "- Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in. \n",
    "\n",
    "- The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. \n",
    "\n",
    "- Thus having the effect of highlighting words that are distinct.\n",
    "\n",
    "- We calculate TF-IDF value of a term as = TF * IDF\n",
    "\n",
    "#### TFIDF \n",
    "- In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "#### Word2Vector format for text analysis \n",
    "- Word2Vec model is used for learning vector representations of words called “word embeddings”. This is typically done as a preprocessing step, after which the learned vectors are fed into a discriminative model (typically an RNN) to generate predictions and perform all sort of interesting things.\n",
    "\n",
    "#### Creating pipelines for analysis\n",
    "\n",
    "#### Text frequency analysis – unigrams, bigrams\n",
    "\n",
    "#### Semantic analysis \n",
    "- The semantic analysis of natural language content starts by reading all of the words in content to capture the real meaning of any text. It identifies the text elements and assigns them to their logical and grammatical role. It also understands the relationships between different concepts in the text.\n",
    "\n",
    "#### Storing processed texts\n",
    "\n",
    "#### Web scraping \n",
    "- Machine learning requires data, lots of data! Import.io offers a scalable, automated way to gather vast amounts of data across hundreds of thousands of web pages. Import.io's web scraping enables machine learning developers to get millions of data points, images, and files for training machine learning models.\n",
    "\n",
    "#### Visualizing text analysis results\n",
    "\n",
    "#### n-gram\n",
    "- that assigns probabilities to sentences and sequences of words, the n-gram. You can think of an N-gram as the sequence of N words, by that notion, a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”\n",
    "\n",
    "#### The Bigram Model\n",
    "- As the name suggests, the bigram model approximates the probability of a word given all the previous words by using only the conditional probability of one preceding word. In other words, you approximate it with the probability: P(the | that)\n",
    "\n",
    "#### Proximity Bigram\n",
    "- Specifically, the initial values of the proximity weights are “1”s instead of random values. Skip-gram is less sensitive to word proximity. In Skip-gram, the neural network input is one single word inside a con- text window around the output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of Text preprocessing\n",
    "    ##### ACCESS\n",
    "    ##### TOCKENIZATION\n",
    "    ##### NORMALIZATION\n",
    "    ##### STOP WORD (SWR)\n",
    "\n",
    "    ##### STEAMMING\n",
    "    ##### POS TAGGING\n",
    "    ##### LEMATIZATION\n",
    "    ##### EMJOI TRICK\n",
    "    ##### nGRAMS\n",
    "    \n",
    "## Text Analytics\n",
    "### Rule Based\n",
    "### Model Based\n",
    "                       Rule Based     ||  Model Based\n",
    "Supervised        ||     X            ||   NB, RF, NN, LSTM, CNN, SVM, HMM\n",
    "\n",
    "UnSupervised      ||  Lexcon          ||    K Means SVD LDA\n",
    "                      Rule\n",
    "                   \n",
    "### PIPE LINE BUILDING                   \n",
    "CORPUS -> LABEL -> LABELLED DATA\n",
    "UnSupervised       Supervised\n",
    "\n",
    "### Following activities are performed\n",
    "1. PRE PROCESSING\n",
    "2. MODELLING\n",
    "3. POST PROECESSING\n",
    "4. PRESENTATION\n",
    "\n",
    "        1             1             1               1\n",
    "#### ACCESS --> TOKENIZE --> STOP WORD --> NORMALIZE       \n",
    "                                                STEM      |       1                2            3           4\n",
    "                                                LEMMA     | => BOW MATRIX  ==> MODELING  ==> PREDICT ==> PRESENT\n",
    "                                                N GRAMS   |                     BUILD\n",
    "                                                POS TAGS  |                     EVALUATE\n",
    "                                                \n",
    "#### EXPLORATION\n",
    "1. PRE PROCESSING\n",
    "2. VISULAIZE \n",
    "\n",
    "    FREQUENCY -- TOP 5% -- 10 %\n",
    "    |\n",
    "    | TABLE ==> | WORD | FREQUECNY |\n",
    "    |\n",
    "    | BAR CHARTS\n",
    "    |\n",
    "    | WORD CLOUD\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"TextProcessing.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"TextProcessing.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bussiness Problems                 Aanlytics Problem   Analytics Solution\n",
    "    1. REVENUE / COST / PROFIT --> MONEY | USE TEXT TO PREDICT OPINIONS | GET TWEETS\n",
    "    2. LOW SPEED / HIGH LATENCY --> TIME | HOW TO PROFILE / PREDICT     | GROUP THEM | K - MEANS / SVD, LSA\n",
    "    3. EXPLODES / POISONOUS --> SAFETY   |                              | ANALYZE THEM\n",
    "    \n",
    "### How to approach\n",
    "    1. GET DATA\n",
    "    2. GROUP RECORDS\n",
    "    3. ANALYIZE\n",
    "    \n",
    "#### ACCESS ==> TOKENYZE ==> NORMALIZE ==> SWR ==> PRE PROCESS(EMICON, STEM, LEMNA) ==> CREATE ANALYIS READY MATRIX (BOW - vector space matix - sparse, TF-IDF - coded matrix , WORD2VEC - embedding)==> GROUP RECORDS (K - MEANS, LDA, SVD) ==> ANALYZE (TREND, RECOMENDATION)\n",
    "\n",
    "### Parameters in K - Means\n",
    "1. k - no of segment\n",
    "2. dh - distance metric\n",
    "3. Inititation - Random, Kmeans ++\n",
    "4. delta - thresho;d\n",
    "5. no of iterartions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"TextClusteringFlow.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"TextClusteringFlow.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "24,August,2019\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "sampleString = \"Hello World\"\n",
    "print(sampleString)\n",
    "timeToday = \"24,August,2019\"\n",
    "print(timeToday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'World']\n",
      "['24', 'August', '2019']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "print(sampleString.split())\n",
    "print(timeToday.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fruit', 'NNP'),\n",
       " ('flies', 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('banana', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pos tagging\n",
    "from nltk.tag import pos_tag\n",
    "pos_tag('Fruit flies like a banana'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
